1, 0.642970025, 74.79%, 74.52%
2, 0.590356774, 80.06%, 79.22%
3, 0.563983888, 80.33%, 79.63%
4, 0.549079859, 80.70%, 79.87%
5, 0.539841316, 80.82%, 79.96%
6, 0.533696577, 80.73%, 79.99%
7, 0.529382710, 80.82%, 80.07%
8, 0.526224519, 80.85%, 80.07%
9, 0.523834972, 81.00%, 80.00%
10, 0.521979065, 81.15%, 80.09%
11, 0.520506907, 81.15%, 80.04%
12, 0.519318742, 80.97%, 80.04%
13, 0.518346083, 80.85%, 79.94%
14, 0.517540230, 80.73%, 79.93%
15, 0.516865748, 80.64%, 79.88%
16, 0.516296276, 80.55%, 79.82%
17, 0.515811849, 80.52%, 79.79%
18, 0.515396998, 80.52%, 79.82%
19, 0.515039683, 80.64%, 79.85%
20, 0.514730347, 80.73%, 79.90%
21, 0.514461266, 80.79%, 79.85%
22, 0.514226224, 80.73%, 79.85%
23, 0.514020194, 80.76%, 79.79%
24, 0.513838941, 80.76%, 79.76%
25, 0.513679009, 80.79%, 79.76%
26, 0.513537470, 80.82%, 79.75%
27, 0.513411918, 80.82%, 79.75%
28, 0.513300251, 80.82%, 79.76%
29, 0.513200725, 80.82%, 79.78%
30, 0.513111808, 80.82%, 79.73%
31, 0.513032256, 80.79%, 79.70%
32, 0.512960953, 80.79%, 79.70%
33, 0.512896941, 80.79%, 79.75%
34, 0.512839376, 80.79%, 79.76%
35, 0.512787551, 80.88%, 79.82%
36, 0.512740793, 80.88%, 79.79%
37, 0.512698596, 80.88%, 79.79%
38, 0.512660456, 80.85%, 79.82%
39, 0.512625954, 80.85%, 79.85%
40, 0.512594703, 80.82%, 79.85%
41, 0.512566371, 80.85%, 79.88%
42, 0.512540674, 80.79%, 79.90%
43, 0.512517336, 80.79%, 79.91%
44, 0.512496126, 80.82%, 79.90%
45, 0.512476853, 80.88%, 79.91%
46, 0.512459312, 80.88%, 79.91%
47, 0.512443337, 80.91%, 79.93%
48, 0.512428787, 80.88%, 79.93%
49, 0.512415524, 80.88%, 79.93%
50, 0.512403426, 80.91%, 79.93%
51, 0.512392398, 80.91%, 79.91%
52, 0.512382354, 80.88%, 79.96%
53, 0.512373162, 80.91%, 79.97%
54, 0.512364786, 80.91%, 79.97%
55, 0.512357145, 80.94%, 79.96%
56, 0.512350147, 80.94%, 79.97%
57, 0.512343760, 80.97%, 79.99%
58, 0.512337922, 80.91%, 79.99%
59, 0.512332616, 80.88%, 79.99%
60, 0.512327749, 80.82%, 79.99%
61, 0.512323292, 80.85%, 79.99%
62, 0.512319232, 80.85%, 79.99%
63, 0.512315507, 80.85%, 79.99%
64, 0.512312113, 80.85%, 80.00%
65, 0.512309044, 80.85%, 80.00%
66, 0.512306201, 80.85%, 80.00%
67, 0.512303603, 80.85%, 79.99%
68, 0.512301242, 80.82%, 79.97%
69, 0.512299089, 80.82%, 79.97%
70, 0.512297123, 80.82%, 79.97%
71, 0.512295330, 80.82%, 79.99%
72, 0.512293676, 80.82%, 79.99%
73, 0.512292202, 80.82%, 79.97%
74, 0.512290854, 80.82%, 79.97%
75, 0.512289626, 80.82%, 79.99%
76, 0.512288490, 80.88%, 79.99%
77, 0.512287449, 80.88%, 79.99%
78, 0.512286537, 80.88%, 79.99%
79, 0.512285685, 80.88%, 80.01%
80, 0.512284929, 80.88%, 80.01%
81, 0.512284236, 80.88%, 80.01%
82, 0.512283609, 80.88%, 80.01%
83, 0.512283039, 80.88%, 80.01%
84, 0.512282510, 80.88%, 80.01%
85, 0.512282064, 80.88%, 80.01%
86, 0.512281637, 80.88%, 80.01%
87, 0.512281271, 80.91%, 80.01%
88, 0.512280940, 80.91%, 80.03%
89, 0.512280629, 80.91%, 80.03%
90, 0.512280345, 80.91%, 80.03%
91, 0.512280115, 80.91%, 80.01%
92, 0.512279912, 80.91%, 80.01%
93, 0.512279697, 80.91%, 80.01%
94, 0.512279529, 80.91%, 80.01%
95, 0.512279376, 80.91%, 80.01%
96, 0.512279259, 80.91%, 80.01%
97, 0.512279133, 80.91%, 80.01%
98, 0.512279050, 80.91%, 80.01%
99, 0.512278959, 80.91%, 80.01%
100, 0.512278871, 80.91%, 80.01%
test accuracy = 80.91%, train accuracy = 80.01%
